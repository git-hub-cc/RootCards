# -*- coding: utf-8 -*-

import json
import os
import shutil
from datetime import datetime

# --- é…ç½®åŒºåŸŸ ---

# å®šä¹‰æ•°æ®ç›®å½•çš„æ ¹è·¯å¾„
DATA_DIR = '../data'

# å®šä¹‰ä¸éœ€è¦å»é‡çš„ç‰¹æ®Šç›®å½•å…³é”®å­—
# è¿™äº›æ˜¯è¯æ ¹ã€å‰ç¼€ã€åç¼€æ‰€åœ¨çš„ç›®å½•
SPECIAL_DIRS = ('/pre/', '/root/', '/suf/')


def create_backup(source_dir: str) -> bool:
    """
    ä¸ºæŒ‡å®šç›®å½•åˆ›å»ºä¸€ä¸ªå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½ã€‚

    Args:
        source_dir (str): éœ€è¦å¤‡ä»½çš„ç›®å½•è·¯å¾„ã€‚

    Returns:
        bool: å¦‚æœå¤‡ä»½æˆåŠŸæˆ–ç”¨æˆ·è·³è¿‡ï¼Œåˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
    """
    if not os.path.isdir(source_dir):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¦å¤‡ä»½çš„ç›®å½• '{source_dir}'ã€‚")
        return False

    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
    backup_dir = f"{source_dir}_backup_{timestamp}"

    # å¢åŠ ç”¨æˆ·äº¤äº’ï¼Œç¡®ä¿å®‰å…¨
    try:
        choice = input(f"âš ï¸ è­¦å‘Š: æ­¤è„šæœ¬å°†ç›´æ¥ä¿®æ”¹æºæ–‡ä»¶ã€‚\n    æ˜¯å¦è¦åˆ›å»ºä¸€ä¸ª '{backup_dir}' å¤‡ä»½ç›®å½•? (Y/n): ").lower().strip()
    except EOFError: # åœ¨æŸäº›éäº¤äº’å¼ç¯å¢ƒä¸­å¯èƒ½ä¼šå‡ºç°
        choice = 'n'

    if choice == 'y' or choice == '':
        try:
            shutil.copytree(source_dir, backup_dir)
            print(f"âœ… æˆåŠŸåˆ›å»ºå¤‡ä»½: '{backup_dir}'")
            return True
        except Exception as e:
            print(f"âŒ å¤‡ä»½å¤±è´¥: {e}")
            return False
    else:
        confirm_no_backup = input("â“ ç¡®å®šè¦åœ¨æ²¡æœ‰å¤‡ä»½çš„æƒ…å†µä¸‹ç»§ç»­å—? è¿™å°†ç›´æ¥è¦†ç›–åŸå§‹æ–‡ä»¶ã€‚ (y/N): ").lower().strip()
        if confirm_no_backup == 'y':
            print("ğŸ‘ å·²è·³è¿‡å¤‡ä»½ï¼Œå°†ç›´æ¥ä¿®æ”¹åŸå§‹æ–‡ä»¶ã€‚")
            return True
        else:
            print("ğŸš« æ“ä½œå·²å–æ¶ˆã€‚")
            return False

def collect_word_data(file_paths: list[str]) -> tuple[dict, dict]:
    """
    é˜¶æ®µ 1: è¯»å–æ‰€æœ‰ JSON æ–‡ä»¶ï¼Œæ”¶é›†æ¯ä¸ªå•è¯çš„æ•°æ®å’Œä½ç½®ä¿¡æ¯ã€‚

    Returns:
        tuple[dict, dict]:
        - word_data_map: {'word': {'canonical_data': {...}, 'locations': [...]}}
        - file_content_map: {'path/to/file.json': { original json content }}
    """
    word_data_map = {}
    file_content_map = {}

    print("\n[é˜¶æ®µ 1/3] ğŸ” æ­£åœ¨æ”¶é›†æ‰€æœ‰æ–‡ä»¶ä¸­çš„å•è¯æ•°æ®...")

    for path in file_paths:
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            file_content_map[path] = data

            is_special = any(sd in path.replace('\\', '/') for sd in SPECIAL_DIRS)

            meanings = data.get('meanings', [])
            if not isinstance(meanings, list):
                continue

            for meaning_idx, meaning_group in enumerate(meanings):
                words_list = meaning_group.get('words', [])
                if not isinstance(words_list, list):
                    continue

                for word_idx, word_item in enumerate(words_list):
                    word = word_item.get('word')
                    if not isinstance(word, str) or not word:
                        continue

                    location_info = {
                        "path": path,
                        "is_special": is_special,
                        "meaning_idx": meaning_idx,
                        "word_idx": word_idx
                    }

                    if word not in word_data_map:
                        word_data_map[word] = {
                            "canonical_data": word_item, # ç¬¬ä¸€ä¸ªé‡åˆ°çš„ç‰ˆæœ¬ä½œä¸ºæƒå¨ç‰ˆæœ¬
                            "locations": [location_info]
                        }
                    else:
                        word_data_map[word]["locations"].append(location_info)
        except Exception as e:
            print(f"âš ï¸ è­¦å‘Š: è¯»å–æ–‡ä»¶ '{path}' å¤±è´¥: {e}ï¼Œå·²è·³è¿‡ã€‚")

    print(f"âœ… æ•°æ®æ”¶é›†å®Œæˆï¼Œå…±ç´¢å¼• {len(word_data_map)} ä¸ªç‹¬ç«‹å•è¯ã€‚")
    return word_data_map, file_content_map


def rebuild_and_write_files(word_data_map: dict, file_content_map: dict, all_paths: list[str]):
    """
    é˜¶æ®µ 2 & 3: åœ¨å†…å­˜ä¸­é‡å»ºæ–‡ä»¶å†…å®¹ï¼Œç„¶åå†™å…¥ç£ç›˜ã€‚
    """
    print("\n[é˜¶æ®µ 2/3] ğŸ§  æ­£åœ¨å†³ç­–éœ€è¦ä¿ç•™å’Œç»Ÿä¸€çš„å•è¯...")

    # åˆ›å»ºåŸå§‹æ–‡ä»¶ç»“æ„çš„æ·±æ‹·è´ï¼Œç”¨äºä¿®æ”¹
    reconstructed_files = {path: json.loads(json.dumps(content)) for path, content in file_content_map.items()}

    # æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶çš„å•è¯åˆ—è¡¨ï¼Œå‡†å¤‡é‡æ–°å¡«å……
    for content in reconstructed_files.values():
        if 'meanings' in content and isinstance(content['meanings'], list):
            for meaning in content['meanings']:
                if 'words' in meaning:
                    meaning['words'] = []

    words_kept_in_regular_files = set()
    total_original_entries = 0
    total_final_entries = 0

    # æŒ‰ç…§åŸå§‹æ–‡ä»¶é¡ºåºéå†ï¼Œä»¥å†³å®šå“ªä¸ªæ˜¯â€œé¦–æ¬¡å‡ºç°â€
    for path in all_paths:
        if path not in file_content_map:
            continue

        original_content = file_content_map[path]
        is_special_file = any(sd in path.replace('\\', '/') for sd in SPECIAL_DIRS)
        words_seen_in_this_file = set() # æ–°å¢ï¼šç”¨äºè·Ÿè¸ªå½“å‰æ–‡ä»¶å†…å·²å‡ºç°çš„å•è¯

        meanings = original_content.get('meanings', [])
        for meaning_idx, meaning_group in enumerate(meanings):
            words_list = meaning_group.get('words', [])
            if not isinstance(words_list, list):
                continue

            for word_item in words_list:
                word = word_item.get('word')
                if not word:
                    continue

                total_original_entries += 1

                # é€»è¾‘è¡¥å……ï¼šå¦‚æœå•è¯åœ¨å½“å‰æ–‡ä»¶ä¸­å·²å‡ºç°ï¼Œåˆ™ç›´æ¥è·³è¿‡ï¼Œå®ç°æ–‡ä»¶å†…å»é‡
                if word in words_seen_in_this_file:
                    continue
                words_seen_in_this_file.add(word)

                if is_special_file:
                    # ç‰¹æ®Šæ–‡ä»¶ä¸­çš„å•è¯æ€»æ˜¯ä¿ç•™å…¶åŸå§‹æ•°æ®
                    reconstructed_files[path]['meanings'][meaning_idx]['words'].append(word_item)
                    total_final_entries += 1
                else:
                    # å¯¹äºæ™®é€šæ–‡ä»¶ï¼Œåªæœ‰å½“è¿™ä¸ªå•è¯æ˜¯é¦–æ¬¡åœ¨æ™®é€šæ–‡ä»¶ä¸­å‡ºç°æ—¶æ‰ä¿ç•™
                    if word not in words_kept_in_regular_files:
                        words_kept_in_regular_files.add(word)
                        # ä½¿ç”¨ word_data_map ä¸­å­˜å‚¨çš„æƒå¨ç‰ˆæœ¬æ•°æ®
                        canonical_data = word_data_map[word]['canonical_data']
                        reconstructed_files[path]['meanings'][meaning_idx]['words'].append(canonical_data)
                        total_final_entries += 1

    words_removed = total_original_entries - total_final_entries
    print(f"âœ… å†³ç­–å®Œæˆã€‚å°†ä¿ç•™ {total_final_entries} ä¸ªæ¡ç›®ï¼Œç§»é™¤ {words_removed} ä¸ªé‡å¤æ¡ç›®ã€‚")

    print("\n[é˜¶æ®µ 3/3] âœï¸  æ­£åœ¨å°†ä¿®æ”¹å†™å…¥æ–‡ä»¶ç³»ç»Ÿ...")
    modified_files_count = 0
    for path, new_content in reconstructed_files.items():
        try:
            # åªæœ‰å½“å†…å®¹å‘ç”Ÿå˜åŒ–æ—¶æ‰å†™å…¥ï¼Œé¿å…ä¸å¿…è¦çš„æ–‡ä»¶ä¿®æ”¹
            if file_content_map.get(path) != new_content:
                with open(path, 'w', encoding='utf-8') as f:
                    json.dump(new_content, f, ensure_ascii=False, indent=2)
                modified_files_count += 1
        except Exception as e:
            print(f"âŒ å†™å…¥æ–‡ä»¶ '{path}' å¤±è´¥: {e}")

    print(f"âœ… å†™å…¥æ“ä½œå®Œæˆï¼Œå…±ä¿®æ”¹äº† {modified_files_count} ä¸ªæ–‡ä»¶ã€‚")
    return words_removed, modified_files_count


def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°
    """
    # 0. å¤‡ä»½
    if not create_backup(DATA_DIR):
        return

    # 1. ç›´æ¥ä½¿ç”¨ç¡¬ç¼–ç çš„æ–‡ä»¶åˆ—è¡¨
    file_paths = [
        'data/middle/pre/re.json', 'data/middle/pre/dis.json', 'data/middle/pre/ex.json', 'data/middle/pre/in.json',
        'data/middle/pre/un.json', 'data/middle/pre/en.json', 'data/middle/pre/com.json', 'data/middle/pre/sub.json',
        'data/middle/pre/trans.json', 'data/middle/pre/pre.json', 'data/middle/pre/de.json', 'data/middle/pre/inter.json',
        'data/middle/pre/ab.json', 'data/middle/pre/sur.json', 'data/middle/pre/dia.json', 'data/middle/pre/op.json',
        'data/high/pre/anti.json', 'data/high/pre/with.json', 'data/middle/suf/tion.json', 'data/middle/suf/ship.json',
        'data/middle/suf/ment.json', 'data/middle/suf/ness.json', 'data/middle/suf/ist.json', 'data/middle/suf/ity.json',
        'data/middle/suf/ess.json', 'data/middle/suf/eer.json', 'data/middle/suf/ance.json', 'data/middle/suf/ure.json',
        'data/middle/suf/al.json', 'data/middle/suf/y.json', 'data/middle/suf/ous.json', 'data/middle/suf/ful.json',
        'data/middle/suf/less.json', 'data/middle/suf/able.json', 'data/middle/suf/ic.json', 'data/middle/suf/ive.json',
        'data/middle/suf/ly.json', 'data/middle/suf/ize.json', 'data/middle/suf/ward.json', 'data/middle/suf/ate.json',
        'data/middle/suf/ish.json', 'data/middle/suf/ary.json', 'data/high/suf/ion.json', 'data/high/suf/ics.json',
        'data/high/suf/logy.json', 'data/middle/root/rect.json', 'data/middle/root/sect.json', 'data/middle/root/flu.json',
        'data/middle/root/tend.json', 'data/middle/root/pos.json', 'data/middle/root/spir.json', 'data/high/root/arm.json',
        'data/high/root/ball.json', 'data/high/root/bear.json', 'data/high/root/bind.json', 'data/high/root/break.json',
        'data/high/root/cept.json', 'data/high/root/cid.json', 'data/high/root/circ.json', 'data/high/root/clar.json',
        'data/high/root/clud.json', 'data/high/root/count.json', 'data/high/root/cru.json', 'data/high/root/cur.json',
        'data/high/root/curs.json', 'data/high/root/dict.json', 'data/high/root/do.json', 'data/high/root/du.json',
        'data/high/root/duc.json', 'data/high/root/equ.json', 'data/high/root/fac.json', 'data/high/root/fend.json',
        'data/high/root/fer.json', 'data/high/root/fid.json', 'data/high/root/fl.json', 'data/high/root/flex.json',
        'data/high/root/gen.json', 'data/high/root/gest.json', 'data/high/root/gl.json', 'data/high/root/graph.json',
        'data/high/root/ject.json', 'data/high/root/lect.json', 'data/high/root/liber.json', 'data/high/root/liter.json',
        'data/high/root/log.json', 'data/high/root/long.json', 'data/high/root/man.json', 'data/high/root/mens.json',
        'data/high/root/ment_mind.json', 'data/high/root/mer.json', 'data/high/root/mid.json', 'data/high/root/min.json',
        'data/high/root/mit.json', 'data/high/root/multi.json', 'data/high/root/nov.json', 'data/high/root/pend.json',
        'data/high/root/pet.json', 'data/high/root/ply.json', 'data/high/root/port.json', 'data/high/root/press.json',
        'data/high/root/rad.json', 'data/high/root/reg.json', 'data/high/root/rupt.json', 'data/high/root/scrib.json',
        'data/high/root/sequ.json', 'data/high/root/serv.json', 'data/high/root/sid.json', 'data/high/root/sign.json',
        'data/high/root/sl.json', 'data/high/root/soci.json', 'data/high/root/solv.json', 'data/high/root/spect.json',
        'data/high/root/spr.json', 'data/high/root/sta.json', 'data/high/root/strict.json', 'data/high/root/struct.json',
        'data/high/root/ten.json', 'data/high/root/terr.json', 'data/high/root/tract.json', 'data/high/root/val.json',
        'data/high/root/vent.json', 'data/high/root/vert.json', 'data/high/root/view.json', 'data/high/root/vinc.json',
        'data/high/root/vis.json', 'data/high/root/voc.json', 'data/high/root/volv.json', 'data/high/root/wr.json',
        'data/middle/geo_world.json', 'data/middle/nature_landscape.json', 'data/middle/city_infrastructure.json',
        'data/middle/weather_seasons.json', 'data/middle/food_ingredients.json', 'data/middle/dining_cooking.json',
        'data/middle/home_bedroom.json', 'data/middle/clothing_appearance.json', 'data/middle/study_hobbies.json',
        'data/middle/people_roles.json', 'data/middle/movement_position.json', 'data/middle/interaction_communication.json',
        'data/middle/mental_emotional.json', 'data/middle/measurement_quantity.json', 'data/middle/attributes_status.json',
        'data/middle/time_logic.json', 'data/middle/month.json', 'data/middle/other.json', 'data/middle/conflict_crisis.json',
        'data/middle/food_nature.json', 'data/middle/society_interaction.json', 'data/middle/thought_education.json',
        'data/middle/objects_tech.json', 'data/middle/emotion_traits.json', 'data/middle/topic_people_roles.json',
        'data/middle/topic_places_locations.json', 'data/middle/topic_food_drink.json', 'data/middle/topic_nature_animals.json',
        'data/middle/topic_objects_tools.json', 'data/middle/topic_action_process.json', 'data/middle/topic_core_concepts.json',
        'data/middle/topic_abstract_qualities.json', 'data/middle/topic_humanities_arts.json',
        'data/middle/topic_conflict_negation.json', 'data/middle/topic_positive_traits.json',
        'data/middle/topic_time_measurement.json', 'data/middle/topic_technology_components.json',
        'data/middle/topic_daily_life.json', 'data/middle/topic_nature.json', 'data/high/etymology_basics.json',
        'data/high/phonetic_rules.json', 'data/high/vocab_a_d.json', 'data/high/vocab_e_h.json', 'data/high/vocab_i_m.json',
        'data/high/vocab_n_p.json', 'data/high/vocab_q_r.json', 'data/high/vocab_s.json', 'data/high/vocab_t.json',
        'data/high/vocab_u_z.json', 'data/CET-4/pre/bene.json', 'data/CET-4/pre/with.json', 'data/CET-4/root/ac.json',
        'data/CET-4/root/ag.json', 'data/CET-4/root/aug.json', 'data/CET-4/root/auto.json', 'data/CET-4/root/bar.json',
        'data/CET-4/root/bat.json', 'data/CET-4/root/cap.json', 'data/CET-4/root/cept.json', 'data/CET-4/root/du.json',
        'data/CET-4/root/form.json', 'data/CET-4/root/fort.json', 'data/CET-4/root/fract.json', 'data/CET-4/root/fus.json',
        'data/CET-4/root/hab.json', 'data/CET-4/root/imper.json', 'data/CET-4/root/main.json', 'data/CET-4/root/mun.json',
        'data/CET-4/root/opt.json', 'data/CET-4/root/pac.json', 'data/CET-4/root/pass.json', 'data/CET-4/root/pater.json',
        'data/CET-4/root/pha.json', 'data/CET-4/root/plaud.json', 'data/CET-4/root/port.json', 'data/CET-4/root/press.json',
        'data/CET-4/root/pro.json', 'data/CET-4/root/rad.json', 'data/CET-4/root/sat.json', 'data/CET-4/root/sen.json',
        'data/CET-4/root/sequ.json', 'data/CET-4/root/sert.json', 'data/CET-4/root/sign.json', 'data/CET-4/root/sol.json',
        'data/CET-4/root/soph.json', 'data/CET-4/root/sta.json', 'data/CET-4/root/tact.json', 'data/CET-4/root/ten.json',
        'data/CET-4/root/tend.json', 'data/CET-4/root/vac.json', 'data/CET-4/root/vol.json', 'data/CET-4/vocab_a_d.json',
        'data/CET-4/vocab_e_h.json', 'data/CET-4/vocab_i_m.json', 'data/CET-4/vocab_n_p.json', 'data/CET-4/vocab_q_r.json',
        'data/CET-4/vocab_s.json', 'data/CET-6/pre/de.json', 'data/CET-6/pre/pro.json', 'data/CET-6/pre/re.json',
        'data/CET-6/pre/sub.json', 'data/CET-6/pre/syn.json', 'data/CET-6/root/fer.json', 'data/CET-6/root/lig.json',
        'data/CET-6/root/part.json', 'data/CET-6/root/pot.json', 'data/CET-6/root/psych.json', 'data/CET-6/root/san.json',
        'data/CET-6/root/sarc.json', 'data/CET-6/root/sat.json', 'data/CET-6/root/sta.json', 'data/CET-6/root/sume.json',
        'data/CET-6/root/tact.json', 'data/CET-6/root/val.json', 'data/CET-6/root/verb.json', 'data/CET-6/root/vert.json',
        'data/CET-6/root/vid.json', 'data/CET-6/vocab_o_q.json', 'data/CET-6/vocab_r_s.json', 'data/CET-6/vocab_t_z.json',
    ]
    print(f"â„¹ï¸  ä½¿ç”¨å†…ç½®çš„æ–‡ä»¶åˆ—è¡¨ï¼Œå…± {len(file_paths)} ä¸ªæ–‡ä»¶ã€‚")

    try:
        # é˜¶æ®µ 1: æ”¶é›†æ•°æ®
        word_data_map, file_content_map = collect_word_data(file_paths)

        # é˜¶æ®µ 2 & 3: é‡å»ºå¹¶å†™å…¥æ–‡ä»¶
        removed_count, modified_count = rebuild_and_write_files(word_data_map, file_content_map, file_paths)

        # 4. è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
        print("\n" + "=" * 40)
        print("ğŸ‰ ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼")
        print("=" * 40)
        print(f"    - ç§»é™¤äº† {removed_count} ä¸ªé‡å¤çš„å•è¯æ¡ç›®ã€‚")
        print(f"    - æ›´æ–°äº† {modified_count} ä¸ª JSON æ–‡ä»¶ã€‚")
        print("\nğŸ’¡ æç¤º: æ‚¨å¯ä»¥æ£€æŸ¥æ–‡ä»¶å†…å®¹ï¼Œæˆ–ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶å·¥å…· (å¦‚ Git) æŸ¥çœ‹å…·ä½“çš„ä¿®æ”¹ã€‚")

    except Exception as e:
        print(f"\nâŒ ç¨‹åºå› æœªçŸ¥é”™è¯¯è€Œç»ˆæ­¢: {e}")


if __name__ == "__main__":
    main()