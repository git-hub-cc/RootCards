# -*- coding: utf-8 -*-
"""
Etymology Visualizer éŸ³é¢‘ç¼“å­˜ç”Ÿæˆè„šæœ¬ (v3.2 - æ–°å¢æ…¢é€ŸéŸ³é¢‘)

åŠŸèƒ½:
1. è‡ªåŠ¨è¯»å– `data/manifest.js` æ–‡ä»¶ï¼Œè·å–æ‰€æœ‰å•è¯æ•°æ®æºã€‚
2. è§£ææ–°çš„JSONæ•°æ®ç»“æ„ï¼Œèƒ½å¤Ÿå¤„ç†å•ä¸ªæ–‡ä»¶å†…åŒ…å«å¤šä¸ª "meanings" (æ„å¢ƒ) æ•°ç»„çš„æƒ…å†µã€‚
3. èšåˆæ‰€æœ‰å”¯ä¸€çš„å•è¯å’Œä¾‹å¥ã€‚
4. ä½¿ç”¨ Google Text-to-Speech (gTTS) æœåŠ¡ï¼Œä¸ºæ¯ä¸ªå•è¯å’Œä¾‹å¥ç”Ÿæˆå¯¹åº”çš„è‹±æ–‡å‘éŸ³ MP3 æ–‡ä»¶ã€‚
5. ã€æ–°å¢åŠŸèƒ½ã€‘ä¸ºæ¯ä¸ªå•è¯é¢å¤–ç”Ÿæˆä¸€ä¸ªæ…¢é€Ÿç‰ˆæœ¬çš„å‘éŸ³æ–‡ä»¶ (æ–‡ä»¶åä»¥ _slow.mp3 ç»“å°¾)ã€‚
6. ä¸ºä¾‹å¥ç”ŸæˆåŸºäºå…¶å†…å®¹çš„æ–‡ä»¶åï¼Œé¿å…å› é¡ºåºå˜åŒ–æˆ–å†…å®¹ç›¸ä¼¼å¯¼è‡´çš„é‡å¤æˆ–å†²çªã€‚
7. å°†ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶ä¿å­˜åˆ° `audio/words` å’Œ `audio/sentences` ç›®å½•ä¸­ã€‚
8. å…·æœ‰ç¼“å­˜æ£€æŸ¥åŠŸèƒ½ï¼šå¦‚æœéŸ³é¢‘æ–‡ä»¶å·²å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œåˆ™ä¼šè·³è¿‡ã€‚

ä½¿ç”¨æ–¹æ³•:
1. ç¡®ä¿å·²å®‰è£… Python 3 å’Œå¿…è¦çš„åº“:
   pip install gtts mutagen
2. å°†æ­¤è„šæœ¬æ”¾ç½®åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ã€‚
3. åœ¨ç»ˆç«¯ä¸­è¿è¡Œæ­¤è„šæœ¬:
   python generate_audio.py
"""

import os
import json
import re
import time
from pathlib import Path
from gtts import gTTS
from mutagen.mp3 import MP3, HeaderNotFoundError

# --- é…ç½®åŒºåŸŸ ---
MANIFEST_PATH = Path("data/manifest.js")
AUDIO_ROOT = Path("audio")
WORDS_DIR = AUDIO_ROOT / "words"
SENTENCES_DIR = AUDIO_ROOT / "sentences"
REQUEST_DELAY = 0.5
MIN_FILE_SIZE_BYTES = 1024  # 1 KB
MAX_FILENAME_SLUG_LENGTH = 60

# --- è„šæœ¬æ ¸å¿ƒé€»è¾‘ ---

def parse_manifest(manifest_path: Path) -> list[Path]:
    """
    è§£æ data-manifest.js æ–‡ä»¶ï¼Œæå–å…¶ä¸­ DATA_FILES æ•°ç»„ä¸­çš„æ‰€æœ‰ JSON æ–‡ä»¶è·¯å¾„ã€‚
    """
    print(f"ğŸ“„ æ­£åœ¨è§£ææ•°æ®æ¸…å•: {manifest_path}")
    if not manifest_path.exists():
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°æ•°æ®æ¸…å•æ–‡ä»¶ '{manifest_path}'ã€‚è¯·ç¡®ä¿è„šæœ¬ä½ç½®æ­£ç¡®ã€‚")
        return []

    try:
        content = manifest_path.read_text(encoding='utf-8')
        match = re.search(r"const DATA_FILES\s*=\s*\[(.*?)\];", content, re.DOTALL)
        if not match:
            print(f"âŒ é”™è¯¯: æ— æ³•åœ¨ '{manifest_path}' ä¸­æ‰¾åˆ° 'DATA_FILES' æ•°ç»„ã€‚")
            return []

        file_paths_str = match.group(1)
        file_paths_str = re.sub(r'//.*', '', file_paths_str)
        paths = [p.strip() for p in re.findall(r"['\"](.*?)['\"]", file_paths_str)]

        absolute_paths = [Path(p) for p in paths if p]
        print(f"   - æˆåŠŸæ‰¾åˆ° {len(absolute_paths)} ä¸ªæ•°æ®æ–‡ä»¶ã€‚")
        return absolute_paths
    except Exception as e:
        print(f"âŒ é”™è¯¯: è§£ææ¸…å•æ–‡ä»¶æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        return []


def aggregate_data(file_paths: list[Path]) -> tuple[set[str], dict[str, list[str]]]:
    """
    éå†æ‰€æœ‰æ•°æ®æ–‡ä»¶ï¼Œèšåˆæ‰€æœ‰å”¯ä¸€çš„å•è¯å’Œä¾‹å¥ã€‚
    """
    unique_words = set()
    unique_sentences = {}
    total_sentence_count = 0

    print("\nğŸ“¦ æ­£åœ¨èšåˆæ‰€æœ‰å•è¯å’Œä¾‹å¥...")
    for file_path in file_paths:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

                if "meanings" not in data or not isinstance(data["meanings"], list):
                    print(f"   - [è­¦å‘Š] æ–‡ä»¶ '{file_path}' ç¼ºå°‘ 'meanings' æ•°ç»„ï¼Œå·²è·³è¿‡ã€‚")
                    continue

                for meaning_group in data["meanings"]:
                    if "words" not in meaning_group or not isinstance(meaning_group["words"], list):
                        continue

                    for word_data in meaning_group["words"]:
                        word = word_data.get("word")
                        sentences_list = word_data.get("sentences")

                        if word:
                            word_lower = word.lower()
                            unique_words.add(word_lower)

                            if sentences_list and isinstance(sentences_list, list):
                                if word_lower not in unique_sentences:
                                    unique_sentences[word_lower] = []

                                for sentence_obj in sentences_list:
                                    if isinstance(sentence_obj, dict) and 'en' in sentence_obj:
                                        sentence_en = sentence_obj['en'].strip()
                                        if sentence_en and sentence_en not in unique_sentences[word_lower]:
                                            unique_sentences[word_lower].append(sentence_en)
                                            total_sentence_count += 1

        except json.JSONDecodeError:
            print(f"   - [è­¦å‘Š] æ–‡ä»¶ '{file_path}' ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ–‡ä»¶ï¼Œå·²è·³è¿‡ã€‚")
        except FileNotFoundError:
            print(f"   - [è­¦å‘Š] æœªæ‰¾åˆ°æ–‡ä»¶ '{file_path}'ï¼Œå·²è·³è¿‡ã€‚")
        except Exception as e:
            print(f"   - [é”™è¯¯] å¤„ç†æ–‡ä»¶ '{file_path}' æ—¶å‡ºé”™: {e}")

    print(f"   - èšåˆå®Œæˆ: æ‰¾åˆ° {len(unique_words)} ä¸ªç‹¬ç«‹å•è¯ï¼Œ{total_sentence_count} æ¡ç‹¬ç«‹ä¾‹å¥ã€‚")
    return unique_words, unique_sentences


def sanitize_for_filename(text: str, max_length: int = MAX_FILENAME_SLUG_LENGTH) -> str:
    """
    å°†æ–‡æœ¬è½¬æ¢ä¸ºä¸€ä¸ªå¯¹æ–‡ä»¶åå®‰å…¨ã€å”¯ä¸€çš„â€œslugâ€ã€‚
    """
    slug = text.lower()
    slug = re.sub(r'[^a-z0-9]+', '_', slug)
    if len(slug) > max_length:
        slug = slug[:max_length]
    slug = slug.strip('_')
    return slug


# ã€æ ¸å¿ƒä¿®æ”¹ã€‘å‡½æ•°ç­¾åå¢åŠ  is_slow å‚æ•°
def generate_audio_file(text: str, output_path: Path, is_slow: bool = False) -> bool:
    """
    ä¸ºç»™å®šçš„æ–‡æœ¬ç”ŸæˆéŸ³é¢‘æ–‡ä»¶ï¼Œå¹¶è¿›è¡Œç¼“å­˜æ£€æŸ¥ã€‚

    :param text: è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬ã€‚
    :param output_path: éŸ³é¢‘æ–‡ä»¶çš„è¾“å‡ºè·¯å¾„ã€‚
    :param is_slow: æ˜¯å¦ç”Ÿæˆæ…¢é€Ÿç‰ˆæœ¬çš„éŸ³é¢‘ã€‚
    :return: ç”ŸæˆæˆåŠŸè¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
    """
    # 1. ç¼“å­˜æ£€æŸ¥ï¼šå¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œåˆ™è·³è¿‡
    if output_path.exists() and output_path.stat().st_size >= MIN_FILE_SIZE_BYTES:
        try:
            MP3(output_path)
            # æ–‡ä»¶æœ‰æ•ˆï¼Œç›´æ¥è¿”å›æˆåŠŸ
            return True
        except HeaderNotFoundError:
            print(f"  [è­¦å‘Š] '{output_path.name}' å·²å­˜åœ¨ä½†æ–‡ä»¶æŸåï¼Œå°†é‡æ–°ç”Ÿæˆã€‚")
        except Exception as e:
            print(f"  [è­¦å‘Š] æ£€æŸ¥ '{output_path.name}' æ—¶å‡ºé”™ ({e})ï¼Œå°†é‡æ–°ç”Ÿæˆã€‚")

    # 2. æ–‡ä»¶ç”Ÿæˆ
    try:
        speed_str = "æ…¢é€Ÿ" if is_slow else "æ­£å¸¸"
        print(f"  [ç”Ÿæˆ] æ­£åœ¨è¯·æ±‚ '{output_path.name}' ({speed_str})...")

        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘æ ¹æ® is_slow å‚æ•°å†³å®š gTTS çš„ slow å±æ€§
        tts = gTTS(text=text, lang='en', slow=is_slow)
        tts.save(str(output_path))

        # åœ¨æ¯æ¬¡è¯·æ±‚åç¨ä½œå»¶è¿Ÿï¼Œé¿å…å¯¹APIé€ æˆè¿‡å¤§å‹åŠ›
        time.sleep(REQUEST_DELAY)
        return True
    except AssertionError:
        print(f"  [é”™è¯¯] æ–‡æœ¬ä¸ºç©ºï¼Œæ— æ³•ä¸º '{output_path.name}' ç”ŸæˆéŸ³é¢‘ã€‚")
        return False
    except Exception as e:
        print(f"  [ä¸¥é‡é”™è¯¯] ç”Ÿæˆ '{output_path.name}' å¤±è´¥: {e}")
        # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œåˆ é™¤å¯èƒ½å·²åˆ›å»ºçš„ç©ºæ–‡ä»¶æˆ–æŸåæ–‡ä»¶ï¼Œç¡®ä¿ä¸‹æ¬¡èƒ½é‡æ–°ç”Ÿæˆ
        if output_path.exists():
            output_path.unlink()
        return False


def main():
    """è„šæœ¬ä¸»æ‰§è¡Œå‡½æ•°"""
    print("=" * 50)
    print("ğŸš€ å¼€å§‹æ‰§è¡Œ Etymology Visualizer éŸ³é¢‘ç¼“å­˜ç”Ÿæˆè„šæœ¬ ğŸš€")
    print("=" * 50)

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    WORDS_DIR.mkdir(parents=True, exist_ok=True)
    SENTENCES_DIR.mkdir(parents=True, exist_ok=True)
    print(f"âœ… è¾“å‡ºç›®å½•å·²å‡†å¤‡å°±ç»ª:\n   - å•è¯: {WORDS_DIR}\n   - ä¾‹å¥: {SENTENCES_DIR}")

    data_files = parse_manifest(MANIFEST_PATH)
    if not data_files:
        print("\nâŒ æœªèƒ½ä»æ¸…å•ä¸­è·å–ä»»ä½•æ•°æ®æ–‡ä»¶ï¼Œè„šæœ¬ç»ˆæ­¢ã€‚")
        return

    words, sentences_map = aggregate_data(data_files)
    if not words and not sentences_map:
        print("\nâŒ æœªèƒ½ä»æ•°æ®æ–‡ä»¶ä¸­èšåˆä»»ä½•å•è¯æˆ–ä¾‹å¥ï¼Œè„šæœ¬ç»ˆæ­¢ã€‚")
        return

    # --- å¼€å§‹ç”Ÿæˆå•è¯éŸ³é¢‘ ---
    print("\n" + "-" * 20)
    print(f"ğŸ¤ å¼€å§‹å¤„ç† {len(words)} ä¸ªå•è¯éŸ³é¢‘ (æ¯è¯2ç§é€Ÿåº¦)...")
    print("-" * 20)
    success_words, failed_words = 0, 0
    total_word_audios = len(words) * 2  # æ¯ä¸ªå•è¯ç”Ÿæˆä¸¤ç§é€Ÿåº¦
    processed_word_audios = 0

    for i, word in enumerate(sorted(list(words)), 1):
        print(f"è¿›åº¦: {i}/{len(words)} - å•è¯: '{word}'")

        # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä¸ºæ¯ä¸ªå•è¯ç”Ÿæˆä¸¤ç§é€Ÿåº¦çš„éŸ³é¢‘ ---
        # 1. ç”Ÿæˆæ­£å¸¸é€Ÿåº¦éŸ³é¢‘
        normal_path = WORDS_DIR / f"{word}.mp3"
        processed_word_audios += 1
        print(f"   ({processed_word_audios}/{total_word_audios})", end="")
        if generate_audio_file(word, normal_path, is_slow=False):
            success_words += 1
        else:
            failed_words += 1

        # 2. ç”Ÿæˆæ…¢é€ŸéŸ³é¢‘
        slow_path = WORDS_DIR / f"{word}_slow.mp3"
        processed_word_audios += 1
        print(f"   ({processed_word_audios}/{total_word_audios})", end="")
        if generate_audio_file(word, slow_path, is_slow=True):
            success_words += 1
        else:
            failed_words += 1

    # --- å¼€å§‹ç”Ÿæˆä¾‹å¥éŸ³é¢‘ (ä¾‹å¥é€šå¸¸åªéœ€æ­£å¸¸é€Ÿåº¦) ---
    total_sentences = sum(len(s) for s in sentences_map.values())
    print("\n" + "-" * 20)
    print(f"ğŸ§ å¼€å§‹å¤„ç† {total_sentences} æ¡ä¾‹å¥éŸ³é¢‘...")
    print("-" * 20)
    success_sentences, failed_sentences = 0, 0
    processed_count = 0
    for word_key in sorted(sentences_map.keys()):
        sentence_list = sentences_map[word_key]
        for index, sentence in enumerate(sentence_list):
            processed_count += 1
            print(f"è¿›åº¦: {processed_count}/{total_sentences} - å•è¯ '{word_key}' çš„ä¾‹å¥ {index + 1}/{len(sentence_list)}")

            sentence_slug = sanitize_for_filename(sentence)
            filename = f"{word_key}_{sentence_slug}.mp3"
            file_path = SENTENCES_DIR / filename

            # ä¾‹å¥é»˜è®¤ä½¿ç”¨æ­£å¸¸é€Ÿåº¦
            if generate_audio_file(sentence, file_path, is_slow=False):
                success_sentences += 1
            else:
                failed_sentences += 1

    # --- æœ€ç»ˆæŠ¥å‘Š ---
    print("\n" + "=" * 50)
    print("ğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼ğŸ‰ğŸ‰ğŸ‰")
    print("=" * 50)
    print("ğŸ“Š ç”ŸæˆæŠ¥å‘Š:")
    print(f"  - å•è¯éŸ³é¢‘: {success_words} ä¸ªæˆåŠŸ, {failed_words} ä¸ªå¤±è´¥ã€‚ (å…±è®¡ {total_word_audios} ä¸ªæ–‡ä»¶)")
    print(f"  - ä¾‹å¥éŸ³é¢‘: {success_sentences} ä¸ªæˆåŠŸ, {failed_sentences} ä¸ªå¤±è´¥ã€‚")
    print("\nç°åœ¨æ‚¨çš„ 'audio' æ–‡ä»¶å¤¹å·²æ˜¯æœ€æ–°çŠ¶æ€ã€‚")
    print("=" * 50)


if __name__ == "__main__":
    main()