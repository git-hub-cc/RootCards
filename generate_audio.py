# -*- coding: utf-8 -*-
"""
Etymology Visualizer éŸ³é¢‘ç¼“å­˜ç”Ÿæˆè„šæœ¬ (v3.1 - ä¼˜åŒ–æ–‡ä»¶å)

åŠŸèƒ½:
1. è‡ªåŠ¨è¯»å– `data/manifest.js` æ–‡ä»¶ï¼Œè·å–æ‰€æœ‰å•è¯æ•°æ®æºã€‚
2. è§£ææ–°çš„JSONæ•°æ®ç»“æ„ï¼Œèƒ½å¤Ÿå¤„ç†å•ä¸ªæ–‡ä»¶å†…åŒ…å«å¤šä¸ª "meanings" (æ„å¢ƒ) æ•°ç»„çš„æƒ…å†µã€‚
3. èšåˆæ‰€æœ‰å”¯ä¸€çš„å•è¯å’Œä¾‹å¥ã€‚
4. ä½¿ç”¨ Google Text-to-Speech (gTTS) æœåŠ¡ï¼Œä¸ºæ¯ä¸ªå•è¯å’Œä¾‹å¥ç”Ÿæˆå¯¹åº”çš„è‹±æ–‡å‘éŸ³ MP3 æ–‡ä»¶ã€‚
5. ã€ä¼˜åŒ–ã€‘ä¸ºä¾‹å¥ç”ŸæˆåŸºäºå…¶å†…å®¹çš„æ–‡ä»¶åï¼Œé¿å…å› é¡ºåºå˜åŒ–æˆ–å†…å®¹ç›¸ä¼¼å¯¼è‡´çš„é‡å¤æˆ–å†²çªã€‚
6. å°†ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶ä¿å­˜åˆ° `audio/words` å’Œ `audio/sentences` ç›®å½•ä¸­ã€‚
7. å…·æœ‰ç¼“å­˜æ£€æŸ¥åŠŸèƒ½ï¼šå¦‚æœéŸ³é¢‘æ–‡ä»¶å·²å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œåˆ™ä¼šè·³è¿‡ã€‚

ä½¿ç”¨æ–¹æ³•:
1. ç¡®ä¿å·²å®‰è£… Python 3 å’Œå¿…è¦çš„åº“:
   pip install gtts mutagen
2. å°†æ­¤è„šæœ¬æ”¾ç½®åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ã€‚
3. åœ¨ç»ˆç«¯ä¸­è¿è¡Œæ­¤è„šæœ¬:
   python generate_audio.py
"""

import os
import json
import re
import time
from pathlib import Path
from gtts import gTTS
from mutagen.mp3 import MP3, HeaderNotFoundError

# --- é…ç½®åŒºåŸŸ ---
MANIFEST_PATH = Path("data/manifest.js")
AUDIO_ROOT = Path("audio")
WORDS_DIR = AUDIO_ROOT / "words"
SENTENCES_DIR = AUDIO_ROOT / "sentences"
REQUEST_DELAY = 0.5
MIN_FILE_SIZE_BYTES = 1024  # 1 KB
MAX_FILENAME_SLUG_LENGTH = 60 # <--- ä¼˜åŒ–ç‚¹: æ–°å¢é…ç½®ï¼Œé™åˆ¶ä¾‹å¥æ–‡ä»¶åç‰‡æ®µçš„æœ€å¤§é•¿åº¦

# --- è„šæœ¬æ ¸å¿ƒé€»è¾‘ ---

def parse_manifest(manifest_path: Path) -> list[Path]:
    """
    è§£æ data-manifest.js æ–‡ä»¶ï¼Œæå–å…¶ä¸­ DATA_FILES æ•°ç»„ä¸­çš„æ‰€æœ‰ JSON æ–‡ä»¶è·¯å¾„ã€‚
    """
    print(f"ğŸ“„ æ­£åœ¨è§£ææ•°æ®æ¸…å•: {manifest_path}")
    if not manifest_path.exists():
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°æ•°æ®æ¸…å•æ–‡ä»¶ '{manifest_path}'ã€‚è¯·ç¡®ä¿è„šæœ¬ä½ç½®æ­£ç¡®ã€‚")
        return []

    try:
        content = manifest_path.read_text(encoding='utf-8')
        match = re.search(r"const DATA_FILES\s*=\s*\[(.*?)\];", content, re.DOTALL)
        if not match:
            print(f"âŒ é”™è¯¯: æ— æ³•åœ¨ '{manifest_path}' ä¸­æ‰¾åˆ° 'DATA_FILES' æ•°ç»„ã€‚")
            return []

        file_paths_str = match.group(1)
        file_paths_str = re.sub(r'//.*', '', file_paths_str)
        paths = [p.strip() for p in re.findall(r"['\"](.*?)['\"]", file_paths_str)]

        absolute_paths = [Path(p) for p in paths if p]
        print(f"   - æˆåŠŸæ‰¾åˆ° {len(absolute_paths)} ä¸ªæ•°æ®æ–‡ä»¶ã€‚")
        return absolute_paths
    except Exception as e:
        print(f"âŒ é”™è¯¯: è§£ææ¸…å•æ–‡ä»¶æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        return []


def aggregate_data(file_paths: list[Path]) -> tuple[set[str], dict[str, list[str]]]:
    """
    éå†æ‰€æœ‰æ•°æ®æ–‡ä»¶ï¼Œèšåˆæ‰€æœ‰å”¯ä¸€çš„å•è¯å’Œä¾‹å¥ã€‚
    ã€æ ¸å¿ƒä¿®æ”¹ã€‘ç°åœ¨ä¼šå¤„ç†æ–°çš„åµŒå¥—å¼JSONç»“æ„ã€‚

    Args:
        file_paths: æ•°æ®æ–‡ä»¶çš„è·¯å¾„åˆ—è¡¨ã€‚

    Returns:
        ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«:
        - unique_words (set): æ‰€æœ‰å”¯ä¸€å•è¯çš„å°å†™é›†åˆã€‚
        - unique_sentences (dict): {å•è¯: [ä¾‹å¥1, ä¾‹å¥2, ...]} çš„å­—å…¸ã€‚
    """
    unique_words = set()
    unique_sentences = {}
    total_sentence_count = 0

    print("\nğŸ“¦ æ­£åœ¨èšåˆæ‰€æœ‰å•è¯å’Œä¾‹å¥...")
    for file_path in file_paths:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

                if "meanings" not in data or not isinstance(data["meanings"], list):
                    print(f"   - [è­¦å‘Š] æ–‡ä»¶ '{file_path}' ç¼ºå°‘ 'meanings' æ•°ç»„ï¼Œå·²è·³è¿‡ã€‚")
                    continue

                for meaning_group in data["meanings"]:
                    if "words" not in meaning_group or not isinstance(meaning_group["words"], list):
                        continue

                    for word_data in meaning_group["words"]:
                        word = word_data.get("word")
                        sentences_list = word_data.get("sentences")

                        if word:
                            word_lower = word.lower()
                            unique_words.add(word_lower)

                            if sentences_list and isinstance(sentences_list, list):
                                if word_lower not in unique_sentences:
                                    unique_sentences[word_lower] = []

                                for sentence_obj in sentences_list:
                                    if isinstance(sentence_obj, dict) and 'en' in sentence_obj:
                                        sentence_en = sentence_obj['en'].strip()
                                        if sentence_en and sentence_en not in unique_sentences[word_lower]:
                                            unique_sentences[word_lower].append(sentence_en)
                                            total_sentence_count += 1

        except json.JSONDecodeError:
            print(f"   - [è­¦å‘Š] æ–‡ä»¶ '{file_path}' ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ–‡ä»¶ï¼Œå·²è·³è¿‡ã€‚")
        except FileNotFoundError:
            print(f"   - [è­¦å‘Š] æœªæ‰¾åˆ°æ–‡ä»¶ '{file_path}'ï¼Œå·²è·³è¿‡ã€‚")
        except Exception as e:
            print(f"   - [é”™è¯¯] å¤„ç†æ–‡ä»¶ '{file_path}' æ—¶å‡ºé”™: {e}")

    print(f"   - èšåˆå®Œæˆ: æ‰¾åˆ° {len(unique_words)} ä¸ªç‹¬ç«‹å•è¯ï¼Œ{total_sentence_count} æ¡ç‹¬ç«‹ä¾‹å¥ã€‚")
    return unique_words, unique_sentences


# <--- ä¼˜åŒ–ç‚¹: æ–°å¢è¾…åŠ©å‡½æ•°ï¼Œç”¨äºå°†å¥å­è½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å "slug"
def sanitize_for_filename(text: str, max_length: int = MAX_FILENAME_SLUG_LENGTH) -> str:
    """
    å°†æ–‡æœ¬è½¬æ¢ä¸ºä¸€ä¸ªå¯¹æ–‡ä»¶åå®‰å…¨ã€å”¯ä¸€çš„â€œslugâ€ã€‚

    1. è½¬æ¢ä¸ºå°å†™ã€‚
    2. å°†æ‰€æœ‰éå­—æ¯æ•°å­—å­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ã€‚
    3. å‹ç¼©è¿ç»­çš„ä¸‹åˆ’çº¿ä¸ºä¸€ä¸ªã€‚
    4. æˆªæ–­åˆ°æœ€å¤§é•¿åº¦ã€‚
    5. æ¸…ç†é¦–å°¾çš„ä¸‹åˆ’çº¿ã€‚
    """
    # è½¬æ¢ä¸ºå°å†™
    slug = text.lower()
    # å°†æ‰€æœ‰éå­—æ¯å’Œéæ•°å­—çš„å­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    slug = re.sub(r'[^a-z0-9]+', '_', slug)
    # æˆªæ–­ä»¥é¿å…æ–‡ä»¶åè¿‡é•¿
    if len(slug) > max_length:
        slug = slug[:max_length]
    # æ¸…ç†å¯èƒ½å‡ºç°åœ¨å¼€å¤´æˆ–ç»“å°¾çš„ä¸‹åˆ’çº¿
    slug = slug.strip('_')
    return slug


def generate_audio_file(text: str, output_path: Path) -> bool:
    """
    ä¸ºç»™å®šçš„æ–‡æœ¬ç”ŸæˆéŸ³é¢‘æ–‡ä»¶ï¼Œå¹¶è¿›è¡Œç¼“å­˜æ£€æŸ¥ã€‚
    """
    if output_path.exists() and output_path.stat().st_size >= MIN_FILE_SIZE_BYTES:
        try:
            MP3(output_path)
            # æ–‡ä»¶å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œè·³è¿‡
            return True
        except HeaderNotFoundError:
            print(f"  [è­¦å‘Š] '{output_path.name}' å·²å­˜åœ¨ä½†æ–‡ä»¶æŸåï¼Œå°†é‡æ–°ç”Ÿæˆã€‚")
        except Exception as e:
            print(f"  [è­¦å‘Š] æ£€æŸ¥ '{output_path.name}' æ—¶å‡ºé”™ ({e})ï¼Œå°†é‡æ–°ç”Ÿæˆã€‚")

    try:
        print(f"  [ç”Ÿæˆ] æ­£åœ¨è¯·æ±‚ '{output_path.name}'...")
        tts = gTTS(text=text, lang='en', slow=False)
        tts.save(str(output_path))
        time.sleep(REQUEST_DELAY)
        return True
    except AssertionError:
        print(f"  [é”™è¯¯] æ–‡æœ¬ä¸ºç©ºï¼Œæ— æ³•ä¸º '{output_path.name}' ç”ŸæˆéŸ³é¢‘ã€‚")
        return False
    except Exception as e:
        print(f"  [ä¸¥é‡é”™è¯¯] ç”Ÿæˆ '{output_path.name}' å¤±è´¥: {e}")
        if output_path.exists():
            output_path.unlink() # åˆ é™¤ç”Ÿæˆå¤±è´¥çš„ç©ºæ–‡ä»¶æˆ–æŸåæ–‡ä»¶
        return False


def main():
    """è„šæœ¬ä¸»æ‰§è¡Œå‡½æ•°"""
    print("=" * 50)
    print("ğŸš€ å¼€å§‹æ‰§è¡Œ Etymology Visualizer éŸ³é¢‘ç¼“å­˜ç”Ÿæˆè„šæœ¬ ğŸš€")
    print("=" * 50)

    WORDS_DIR.mkdir(parents=True, exist_ok=True)
    SENTENCES_DIR.mkdir(parents=True, exist_ok=True)
    print(f"âœ… è¾“å‡ºç›®å½•å·²å‡†å¤‡å°±ç»ª:\n   - å•è¯: {WORDS_DIR}\n   - ä¾‹å¥: {SENTENCES_DIR}")

    data_files = parse_manifest(MANIFEST_PATH)
    if not data_files:
        print("\nâŒ æœªèƒ½ä»æ¸…å•ä¸­è·å–ä»»ä½•æ•°æ®æ–‡ä»¶ï¼Œè„šæœ¬ç»ˆæ­¢ã€‚")
        return

    words, sentences_map = aggregate_data(data_files)
    if not words and not sentences_map:
        print("\nâŒ æœªèƒ½ä»æ•°æ®æ–‡ä»¶ä¸­èšåˆä»»ä½•å•è¯æˆ–ä¾‹å¥ï¼Œè„šæœ¬ç»ˆæ­¢ã€‚")
        return

    # --- å¼€å§‹ç”Ÿæˆå•è¯éŸ³é¢‘ ---
    print("\n" + "-" * 20)
    print(f"ğŸ¤ å¼€å§‹å¤„ç† {len(words)} ä¸ªå•è¯éŸ³é¢‘...")
    print("-" * 20)
    success_words, failed_words = 0, 0
    for i, word in enumerate(sorted(list(words)), 1):
        print(f"è¿›åº¦: {i}/{len(words)} - å•è¯: '{word}'")
        file_path = WORDS_DIR / f"{word}.mp3"
        if generate_audio_file(word, file_path):
            success_words += 1
        else:
            failed_words += 1

    # --- å¼€å§‹ç”Ÿæˆä¾‹å¥éŸ³é¢‘ ---
    total_sentences = sum(len(s) for s in sentences_map.values())
    print("\n" + "-" * 20)
    print(f"ğŸ§ å¼€å§‹å¤„ç† {total_sentences} æ¡ä¾‹å¥éŸ³é¢‘...")
    print("-" * 20)
    success_sentences, failed_sentences = 0, 0
    processed_count = 0
    for word_key in sorted(sentences_map.keys()):
        sentence_list = sentences_map[word_key]
        for index, sentence in enumerate(sentence_list):
            processed_count += 1
            print(f"è¿›åº¦: {processed_count}/{total_sentences} - å•è¯ '{word_key}' çš„ä¾‹å¥ {index + 1}/{len(sentence_list)}")

            # <--- ä¼˜åŒ–ç‚¹: ä½¿ç”¨æ–°çš„å‡½æ•°ç”ŸæˆåŸºäºå†…å®¹çš„æ–‡ä»¶å
            sentence_slug = sanitize_for_filename(sentence)
            filename = f"{word_key}_{sentence_slug}.mp3"
            # --->

            file_path = SENTENCES_DIR / filename
            if generate_audio_file(sentence, file_path):
                success_sentences += 1
            else:
                failed_sentences += 1

    # --- æœ€ç»ˆæŠ¥å‘Š ---
    print("\n" + "=" * 50)
    print("ğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼ğŸ‰ğŸ‰ğŸ‰")
    print("=" * 50)
    print("ğŸ“Š ç”ŸæˆæŠ¥å‘Š:")
    print(f"  - å•è¯éŸ³é¢‘: {success_words} ä¸ªæˆåŠŸ, {failed_words} ä¸ªå¤±è´¥ã€‚")
    print(f"  - ä¾‹å¥éŸ³é¢‘: {success_sentences} ä¸ªæˆåŠŸ, {failed_sentences} ä¸ªå¤±è´¥ã€‚")
    print("\nç°åœ¨æ‚¨çš„ 'audio' æ–‡ä»¶å¤¹å·²æ˜¯æœ€æ–°çŠ¶æ€ã€‚")
    print("=" * 50)


if __name__ == "__main__":
    main()